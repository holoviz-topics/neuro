{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Viewer\n",
    "![status](https://img.shields.io/badge/status-in%20progress-orange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./assets/230524_eeg-viewer.png\" alt=\"eeg viewer preview\" width=\"450\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This workflow is intended to demonstrate the visualization of a set of 1D EEG timeseries with HoloViz and Bokeh tools.\n",
    "\n",
    "For details specific to this workflow, such as goals, specifications, and bottlenecks, please see this workflow's [readme](./readme_eeg-viewer.md).\n",
    "\n",
    "For a summary of EEG research, data, and software, please see [neuro/wiki/EEG-notes](https://github.com/holoviz-topics/neuro/wiki/EEG-notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Requirements</p>\n",
    "    <p>This workflow notebook requires the <a href=\"./environment.yml\">environment</a> specified in this workflow directory.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "import holoviews as hv; hv.extension('bokeh')\n",
    "from holoviews.plotting.links import RangeToolLink\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews import Dataset\n",
    "\n",
    "from neurodatagen.eeg import generate_eeg_powerlaw\n",
    "from hvneuro import download_file\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "from scipy.stats import zscore\n",
    "import panel as pn; pn.extension(template='material')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data pipeline\n",
    "\n",
    "The `generate_eeg_powerlaw` function synthesizes EEG data as high-pass filtered pink noise power law time series by default. The function returns a 2D numpy array of synthetic EEG data (in microvolts) shaped as (number of channels, total samples), a 1D time array (in seconds), and a list of channel names. Parameters such as the high-pass filter factor (in Hz) and an amplitude scaling factor allow customization of the generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 25\n",
    "n_seconds = 30\n",
    "fs = 512\n",
    "\n",
    "data, time, channels = generate_eeg_powerlaw(n_channels, n_seconds, fs)\n",
    "\n",
    "print(f'shape: {data.shape} (n_channels, samples) ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_ch_disp = 10  # max channels to initially display\n",
    "max_t_disp = 3 # max time in seconds to initially display\n",
    "\n",
    "spacing = 5.5  # Spacing between channels\n",
    "offset = np.std(data) * spacing\n",
    "\n",
    "annotation = hv.VSpan(1, 2) # example annotation (start, end) time\n",
    "\n",
    "# Create a hv.Curve element per chan\n",
    "channel_curves = []\n",
    "max_data = data.max()\n",
    " \n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Channel\", \"@channel\"),\n",
    "    (\"Time\", \"$x s\"),\n",
    "    (\"Amplitude\", \"@original_amplitude µV\")])\n",
    "\n",
    "for i, channel_data in enumerate(data):\n",
    "    offset_data = channel_data + (i * offset)\n",
    "    max_data = max(offset_data.max(), max_data) # update max\n",
    "    ds = Dataset((time, offset_data, channel_data, channels[i]), [\"Time\", \"Amplitude\", \"original_amplitude\", \"channel\"])\n",
    "    channel_curves.append(\n",
    "        hv.Curve(ds, \"Time\", [\"Amplitude\", \"original_amplitude\", \"channel\"]).opts(\n",
    "            color=\"black\", line_width=1,\n",
    "            tools=[hover, 'xwheel_zoom'], shared_axes=False))\n",
    "\n",
    "# Create mapping from yaxis location to ytick for each channel\n",
    "# so we can have categorical-style labeling on a continuous axis.\n",
    "# Note: this would/should change when we implement independent \n",
    "# coordinates.\n",
    "yticks = [(i * offset, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "# Create an overlay of curves\n",
    "# TODO.. setting x/y_range bounds does not yet restrict the RangeTool from going beyond these limits\n",
    "# TODO.. the zoom out will stop when it hits any single bound, and not continue zooming out in other directions/dims\n",
    "eeg_viewer = (annotation * hv.Overlay(channel_curves, kdims=\"Channel\")).opts(\n",
    "    padding=0, xlabel=\"Time (s)\", ylabel=\"Channel\", #default_tools=['hover', 'pan', 'box_zoom', 'save', 'reset'],\n",
    "    yticks=yticks, show_legend=False, aspect=1.5, responsive=True,\n",
    "    shared_axes=False, backend_opts={\n",
    "        \"x_range.bounds\": (time.min(), time.max()),\n",
    "        \"y_range.bounds\": (data.min(), max_data)})\n",
    "\n",
    "# Get the y positions of the yticks to use as yaxis of minimap image\n",
    "y_positions, _ = zip(*yticks)\n",
    "\n",
    "# Compute z-scores across time for each channel\n",
    "z_data = zscore(data, axis=1)\n",
    "\n",
    "# Generate the zscored image for the minimap using the y tiack positions from the eeg_viewer\n",
    "minimap = rasterize(hv.Image((time, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\"))\n",
    "\n",
    "# Style the minimap \n",
    "clim_mul = 1.2\n",
    "minimap = minimap.opts(\n",
    "    cmap=\"RdBu_r\", colorbar=False, xlabel='', alpha=.5, yticks=[yticks[0], yticks[-1]],\n",
    "    height=100, responsive=True, default_tools=[''], shared_axes=False, clim=(-z_data.std()*clim_mul, z_data.std()*clim_mul))\n",
    "    \n",
    "# Create RangeToolLink between the minimap and the main EEG viewer \n",
    "# (quirk: apply to just one eeg trace and it will apply to all. see HoloViews #4472)\n",
    "max_y_disp = np.max(data[max_ch_disp-1,:] + ((max_ch_disp-1) * offset))\n",
    "RangeToolLink(minimap, list(eeg_viewer.values())[0], axes=[\"x\", \"y\"],\n",
    "              boundsx=(None, max_t_disp),\n",
    "              boundsy=(None, max_y_disp))\n",
    "\n",
    "\n",
    "# layout = (eeg_viewer + minimap).cols(1).opts(shared_axes=False, merge_tools=False)\n",
    "# eeg_app = pn.Row(layout).servable() # too much spacing between plots in served app\n",
    "# eeg_app = pn.Column(pn.Row(eeg_viewer, min_height=500, sizing_mode='stretch_both'), minimap, sizing_mode='stretch_both')#.servable()#target='main') # BUG Panel #5315: rangetool is variably active in the bokeh toolbar on eeg viewer plot.. not respecting shared_axes=False\n",
    "\n",
    "# reverting approach because of the rangetool bug.. will deal with the spacing in the served app later\n",
    "eeg_app = pn.Column((eeg_viewer + minimap * annotation).cols(1), min_height=650).servable(target='main', title='EEG Viewer with HoloViz and Bokeh')\n",
    "eeg_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is 2.6 MB on disk\n",
    "url = \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R04.edf?download\"\n",
    "local_data_path = \"../../data/\"\n",
    "\n",
    "# Will not download if already present at local_data_path\n",
    "local_file_path = download_file(url, local_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_edf(local_file_path, preload=True)\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview the channel names, types, signal ranges, and uncompressed size\n",
    "raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean channel names, set sensor positions, and reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the channel names\n",
    "raw.rename_channels(lambda s: s.strip(\".\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # preview available montages that are shipped with MNE\n",
    "# mne.channels.get_builtin_montages(descriptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use the standard 10-20\n",
    "# montage = mne.channels.make_standard_montage(\"standard_1020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the assigned positions of our data channels\n",
    "# raw.set_montage(montage, match_case=False)\n",
    "# sphere=(0, 0.015, 0, 0.099) #manually adjust the y origin coord and radius a bit\n",
    "# raw.plot_sensors(show_names=True, sphere=sphere);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-reference EEG data to the average over all recording channels\n",
    "raw.set_eeg_reference(\"average\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the data for plotting into simple arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = raw.times\n",
    "channels = raw.ch_names\n",
    "\n",
    "# get the EEG data (for this data set, all channels are EEG anyways)\n",
    "eeg_indices = mne.pick_types(raw.info, eeg=True)\n",
    "data = raw.get_data(picks=eeg_indices, units={\"eeg\":\"uV\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ch_disp = 10  # max channels to initially display\n",
    "max_t_disp = 5 # max time in seconds to initially display\n",
    "\n",
    "spacing = 2.5  # Spacing between channels\n",
    "offset = np.std(data) * spacing\n",
    "\n",
    "# Create a hv.Curve element per channel\n",
    "channel_curves = []\n",
    "max_data = data.max()\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Channel\", \"@channel\"),\n",
    "    (\"Time\", \"$x s\"),\n",
    "    (\"Amplitude\", \"@original_amplitude µV\")])\n",
    "for i, channel_data in enumerate(data):\n",
    "    offset_data = channel_data + (i * offset)\n",
    "    max_data = max(offset_data.max(), max_data)  # update max\n",
    "    ds = Dataset((time, offset_data, channel_data, channels[i]), [\"Time\", \"Amplitude\", \"original_amplitude\", \"channel\"])\n",
    "    channel_curves.append(\n",
    "        hv.Curve(ds, \"Time\", [\"Amplitude\", \"original_amplitude\", \"channel\"]).opts(\n",
    "            color=\"black\", line_width=1,\n",
    "            tools=[hover, 'xwheel_zoom'], shared_axes=False))\n",
    "\n",
    "# Create mapping from yaxis location to ytick for each channel\n",
    "yticks = [(i * offset, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "# Create an overlay of curves\n",
    "eeg_viewer = hv.Overlay(channel_curves, kdims=\"Channel\").opts(\n",
    "    padding=0, xlabel=\"Time (s)\", ylabel=\"Channel\", yticks=yticks, show_legend=False, aspect=1.5, responsive=True,\n",
    "    shared_axes=False, backend_opts={\n",
    "        \"x_range.bounds\": (time.min(), time.max()),\n",
    "        \"y_range.bounds\": (data.min(), max_data)})\n",
    "\n",
    "# Get the y positions of the yticks to use as yaxis of minimap image\n",
    "y_positions, _ = zip(*yticks)\n",
    "\n",
    "# Compute z-scores across time for each channel\n",
    "z_data = zscore(data, axis=1)\n",
    "\n",
    "# Generate the zscored image for the minimap using the y tick positions from the eeg_viewer\n",
    "minimap = rasterize(hv.Image((time, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\"))\n",
    "minimap = minimap.opts(\n",
    "    cmap=\"RdBu_r\", colorbar=False, xlabel='', alpha=.5, yticks=[yticks[0], yticks[-1]],\n",
    "    height=100, responsive=True, default_tools=[''], shared_axes=False, clim=(-z_data.std()*2.5, z_data.std()*2.5))\n",
    "\n",
    "# Create RangeToolLink between the minimap and the main EEG viewer \n",
    "if len(channels) < max_ch_disp:\n",
    "    max_ch_disp = len(channels)\n",
    "max_y_disp = np.max(data[max_ch_disp-1,:] + ((max_ch_disp-1) * offset))\n",
    "\n",
    "time_s = len(time)/raw.info['sfreq']\n",
    "if time_s < max_t_disp:\n",
    "    max_t_disp = time_s\n",
    "    \n",
    "RangeToolLink(minimap, list(eeg_viewer.values())[0], axes=[\"x\", \"y\"],\n",
    "              boundsx=(None, max_t_disp),\n",
    "              boundsy=(None, max_y_disp))\n",
    "\n",
    "eeg_app = pn.Column((eeg_viewer + minimap).cols(1), min_height=650) #.servable(target='main')\n",
    "eeg_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
